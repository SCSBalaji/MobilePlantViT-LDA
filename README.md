# MobilePlantViT-LDA

A Lightweight Hybrid CNN-Transformer Architecture for Plant Disease Classification with Linear Differential Attention

---

## üåø Overview

**MobilePlantViT-LDA** is a novel lightweight deep learning architecture designed for efficient plant disease classification on mobile and edge devices. It combines the local feature extraction capabilities of MobileNet-inspired CNNs with the global context understanding of Vision Transformers (ViT), featuring a custom **Linear Differential Attention (LDA)** mechanism for improved noise cancellation and feature extraction.

### Key Features

- **Hybrid Architecture**: Combines efficient CNN backbone with transformer attention
- **Linear Differential Attention**: Novel attention mechanism that computes the difference between two attention maps for noise cancellation
- **Mobile-First Design**: Optimized for deployment on resource-constrained devices (<5M parameters)
- **Multiple Variants**: Tiny (~220K), Small (~490K), Base (~867K), and Large (~1.9M) parameter configurations
- **Comprehensive Pipeline**: End-to-end training, evaluation, and deployment workflow

---

## üèóÔ∏è Architecture

```markdown
Input (224√ó224√ó3)
       ‚îÇ
       ‚ñº CNN Stage
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    GhostConv     ‚îÇ  ‚Üí Efficient feature generation with ghost features
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Fused-IR Block ‚îÇ  ‚Üí Fused inverted residual for spatial processing
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Coordinate Attn  ‚îÇ  ‚Üí Position-aware channel attention
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº Transition Stage
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Patch Embedding ‚îÇ  ‚Üí Convert spatial features to sequence
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Positional Enc.  ‚îÇ  ‚Üí Add position information
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº Transformer Stage
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ       LDA        ‚îÇ  ‚Üí Linear Differential Attention
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Residual LN    ‚îÇ  ‚Üí LayerNorm with skip connection
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Bottleneck FFN  ‚îÇ  ‚Üí Parameter-efficient feed-forward
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº Classifier Stage
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ       GAP        ‚îÇ  ‚Üí Global Average Pooling
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Classifier Head ‚îÇ  ‚Üí Final classification
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
Output (38 classes)
```



### Linear Differential Attention (LDA)

The core innovation of this architecture is the **Linear Differential Attention** mechanism:

```markdown
A_diff = Œ± √ó (softmax(Q‚ÇÅK‚ÇÅ·µÄ) - softmax(Q‚ÇÇK‚ÇÇ·µÄ))
```



This differential approach:
- Cancels out noise common to both attention maps
- Enhances meaningful patterns that differ between maps
- Provides learnable noise cancellation via the Œ± parameter

---

## üìä Model Variants

| Variant | Parameters | embed_dim | num_heads | Use Case |
|---------|------------|-----------|-----------|----------|
| **Tiny** | ~220K | 128 | 4 | Edge devices, IoT, real-time inference |
| **Small** | ~490K | 192 | 6 | Mobile apps, balanced performance |
| **Base** | ~867K | 256 | 8 | Default choice, best accuracy/size trade-off |
| **Large** | ~1.9M | 384 | 12 | Maximum accuracy, server deployment |

---

## üìÅ Project Structure

```markdown
MobilePlantViT-LDA/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ blocks/                    # Neural network building blocks
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ attention.py           # Linear Differential Attention
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ classifier.py          # GAP and Classification Head
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ coord_attention.py     # Coordinate Attention module
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ffn.py                 # Bottleneck Feed-Forward Network
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fused_ir.py            # Fused Inverted Residual Block
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ghost_conv.py          # Ghost Convolution
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ patch_embed.py         # Patch Embedding
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ positional_encoding.py # Sinusoidal Positional Encoding
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils.py               # Utility functions
‚îÇ   ‚îî‚îÄ‚îÄ models/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ mobile_plant_vit.py    # Main model implementation
‚îú‚îÄ‚îÄ preprocessing/
‚îÇ   ‚îî‚îÄ‚îÄ preprocessing_color.ipynb  # Data preprocessing pipeline
‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îî‚îÄ‚îÄ training-color-mobileplantvit-large.ipynb  # Training notebook
‚îî‚îÄ‚îÄ README.md
```



---

## üöÄ Quick Start

### Installation

```bash
git clone https://github.com/yourusername/MobilePlantViT-LDA.git
cd MobilePlantViT-LDA
pip install torch torchvision numpy matplotlib pillow tqdm scikit-learn
```

<!-- need to add more -->

<!-- üëálast part added already -->
---

## üî¨ Technical Details

### Attention Mechanism

The Linear Differential Attention computes:

1. Project input to Q‚ÇÅ, Q‚ÇÇ, K‚ÇÅ, K‚ÇÇ, V
2. Compute attention scores: `A‚ÇÅ = softmax(Q‚ÇÅK‚ÇÅ·µÄ/‚àöd)`, `A‚ÇÇ = softmax(Q‚ÇÇK‚ÇÇ·µÄ/‚àöd)`
3. Differential attention: `A_diff = Œ± √ó (A‚ÇÅ - A‚ÇÇ)`
4. Apply to values: `output = A_diff √ó V`

### Parameter Efficiency

The architecture achieves parameter efficiency through:
- **GhostConv**: Generates features cheaply via depthwise operations
- **Bottleneck FFN**: Contracts then expands (opposite of standard transformer)
- **Single Transformer Block**: Minimal transformer overhead
- **Efficient Projections**: Careful dimensionality choices

---

## üìö References

- [GhostNet: More Features from Cheap Operations](https://arxiv.org/abs/1911.11907)
- [Coordinate Attention for Efficient Mobile Network Design](https://arxiv.org/abs/2103.02907)
- [EfficientNetV2: Smaller Models and Faster Training](https://arxiv.org/abs/2104.00298)
- [DIFF Transformer](https://arxiv.org/abs/2410.05258)
- [An Image is Worth 16x16 Words (ViT)](https://arxiv.org/abs/2010.11929)

---

## üìÑ License

This project is licensed under the MIT License - see the LICENSE file for details.

---

## ü§ù Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## üìß Contact

For questions or feedback, please open an issue on GitHub.

---

## ‚≠ê Acknowledgments

- PlantVillage dataset for providing the plant disease images
- The PyTorch team for the excellent deep learning framework
- The research community for the foundational architectures and techniques